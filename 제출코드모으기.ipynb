{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb4c0a-6edd-4eac-8148-d7b29ffa9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "# CSV 파일 경로\n",
    "csv_file_path = 'C:/Users/82102/Desktop/rain/pyrain/data/rainfall_train1.csv'\n",
    "# 데이터베이스 파일 경로\n",
    "db_file_path = 'C:/Users/82102/Desktop/rain/pyrain/data/qwer.db'\n",
    "# CSV 파일을 DataFrame으로 읽기\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# -999 값을 결측치(NaN)로 대체\n",
    "df['rainfall_train.class_interval'] = df['rainfall_train.class_interval'].replace(-999, np.nan)\n",
    "\n",
    "# 선형 보간법을 사용하여 결측치를 채우고, 소수점을 반올림\n",
    "df['rainfall_train.class_interval'] = df['rainfall_train.class_interval'].interpolate(method='linear').round()\n",
    "\n",
    "# 결측치가 제대로 처리되었는지 확인\n",
    "class_interval_counts_after = df['rainfall_train.class_interval'].value_counts().sort_index()\n",
    "print(\"\\n결측치 처리 후 class_interval의 종류와 각 종류의 개수:\")\n",
    "print(class_interval_counts_after)\n",
    "\n",
    "# 처리된 데이터를 데이터베이스에 다시 저장\n",
    "conn = sqlite3.connect(db_file_path)\n",
    "df.to_sql('qwert', conn, if_exists='replace', index=False)\n",
    "conn.close()\n",
    "\n",
    "# class_interval 종류별 개수 출력\n",
    "print(\"\\n결측치 처리 후 종류별 class_interval의 개수:\")\n",
    "print(class_interval_counts_after)\n",
    "\n",
    "# class_interval 종류별 개수를 데이터프레임으로 변환하여 출력\n",
    "class_interval_counts_df = class_interval_counts_after.reset_index()\n",
    "class_interval_counts_df.columns = ['class_interval', 'count']\n",
    "print(\"\\nclass_interval 종류와 개수를 데이터프레임 형태로 출력:\")\n",
    "print(class_interval_counts_df)\n",
    "\n",
    "\n",
    "print(\"\\n결측치가 성공적으로 처리되고 데이터베이스에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d970a20e-2e23-40d3-a1df-632c1eca48d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "기상 데이터에서 결측치는 모델의 패턴 학습에 큰 영향을 미친다. 하지만 결측치를 삭제 할 시에 정보손실과 패턴의 손상을 최소화 하기위해 결측치를 nan으로 대체후 선형보간법을 이용해서 결측치를 처리하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5f2e82-4c53-478f-b889-970a19064d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# 데이터 로드 함수\n",
    "def load_data(db_path, table_name):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    data = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    return data\n",
    "\n",
    "# 파생 변수 생성 함수\n",
    "def create_features(data):\n",
    "    # 컬럼명 변경\n",
    "    data = data.rename(columns={\n",
    "        'rainfall_train.ef_month': 'ef_month',\n",
    "        'rainfall_train.fc_hour': 'fc_hour',\n",
    "        'rainfall_train.ef_hour': 'ef_hour',\n",
    "        'rainfall_train.fc_day': 'fc_day',\n",
    "        'rainfall_train.ef_day': 'ef_day',\n",
    "        'rainfall_train.fc_month' : 'fc_month',\n",
    "        'rainfall_train.v01':'v01',\n",
    "        'rainfall_train.v02':'v02',\n",
    "        'rainfall_train.v03':'v03', \n",
    "        'rainfall_train.v04':'v04',\n",
    "        'rainfall_train.v05':'v05',\n",
    "        'rainfall_train.v06':'v06', \n",
    "        'rainfall_train.v07':'v07',\n",
    "        'rainfall_train.v08':'v08', \n",
    "        'rainfall_train.v09':'v09',\n",
    "        'rainfall_train.dh' : 'dh'\n",
    "    })\n",
    "\n",
    "    # 파생 변수 생성\n",
    "    data['fc_ef_day_diff'] = data['ef_day'] - data['fc_day']\n",
    "    data['ef_hour_sin'] = np.sin(2 * np.pi * data['ef_hour'] / 24)\n",
    "    data['ef_hour_cos'] = np.cos(2 * np.pi * data['ef_hour'] / 24)\n",
    "    data['fc_ef_day_ratio'] = np.where(data['ef_day'] != 0, data['fc_day'] / data['ef_day'], 0)\n",
    "    data['fc_ef_hour_diff'] = data['ef_hour'] - data['fc_hour']\n",
    "    data['fc_ef_hour_ratio'] = np.where(data['ef_hour'] != 0, data['fc_hour'] / data['ef_hour'], 0)\n",
    "    data['fc_ef_month_diff'] = data['ef_month'] - data['fc_month']\n",
    "    return data\n",
    "\n",
    "# 데이터 저장 함수\n",
    "def save_data_to_db(data, db_path, table_name):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    data.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    conn.close()\n",
    "\n",
    "# 메인 함수\n",
    "def main():\n",
    "    db_path = 'C:/Users/82102/Desktop/rain/pyrain/data/qwer.db'\n",
    "    table_name = 'qwert'\n",
    "    \n",
    "    # 데이터 로드\n",
    "    print(\"Loading data...\")\n",
    "    data = load_data(db_path, table_name)\n",
    "    \n",
    "    # 파생 변수 생성\n",
    "    print(\"Creating features...\")\n",
    "    data = create_features(data)\n",
    "    \n",
    "    # 데이터 저장\n",
    "    print(\"Saving data to database...\")\n",
    "    save_data_to_db(data, db_path, table_name)\n",
    "    print(\"Data saved successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb60b66e-9968-45ca-ad5a-2a73b9a41144",
   "metadata": {},
   "outputs": [],
   "source": [
    "이후에 사용할 test코드에서의 변수 통일성을 위해서 rainfall_train의 접두사를 모두 제거하고 파생변수를 생성하였다.\n",
    "파생변수에 대해서는 추후에 설명하도록한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9116b05e-314f-48b3-8705-b488a1882e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 데이터베이스 테이블의 열 이름과 열 수를 확인하는 함수\n",
    "def get_column_names_and_count(db_path, table_name):\n",
    "    print(\"Retrieving column names and count from the database.\")\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        query = f\"SELECT * FROM {table_name} LIMIT 1\"\n",
    "        data = pd.read_sql(query, conn)\n",
    "        column_names = data.columns.tolist()\n",
    "        column_count = len(column_names)\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving column names and count: {e}\")\n",
    "        return None, None\n",
    "    finally:\n",
    "        conn.close()\n",
    "    print(\"Successfully retrieved column names and count.\")\n",
    "    return column_names, column_count\n",
    "\n",
    "# 구간 확률 계산 함수\n",
    "def calculate_segment_probabilities(data):\n",
    "    segment_prob_1 = data['v01'] - data['v02']\n",
    "    segment_prob_2 = data['v02'] - data['v03']\n",
    "    segment_prob_3 = data['v03'] - data['v04']\n",
    "    segment_prob_4 = data['v04'] - data['v05']\n",
    "    segment_prob_5 = data['v05'] - data['v06']\n",
    "    segment_prob_6 = data['v06'] - data['v07']\n",
    "    segment_prob_7 = data['v07'] - data['v08']\n",
    "    segment_prob_8 = data['v08'] - data['v09']\n",
    "    segment_prob_9 = data['v09']\n",
    "    \n",
    "    segment_prob_df = pd.DataFrame({\n",
    "        'segment_prob_1': segment_prob_1,\n",
    "        'segment_prob_2': segment_prob_2,\n",
    "        'segment_prob_3': segment_prob_3,\n",
    "        'segment_prob_4': segment_prob_4,\n",
    "        'segment_prob_5': segment_prob_5,\n",
    "        'segment_prob_6': segment_prob_6,\n",
    "        'segment_prob_7': segment_prob_7,\n",
    "        'segment_prob_8': segment_prob_8,\n",
    "        'segment_prob_9': segment_prob_9\n",
    "    })\n",
    "    \n",
    "    # segment_prob_zero 계산\n",
    "    segment_prob_df['segment_prob_zero'] = 100 - segment_prob_df[['segment_prob_1', 'segment_prob_2', 'segment_prob_3', 'segment_prob_4', 'segment_prob_5', 'segment_prob_6', 'segment_prob_7', 'segment_prob_8', 'segment_prob_9']].sum(axis=1)\n",
    "    \n",
    "    return segment_prob_df\n",
    "\n",
    "# 월별 평균 강수량과 강수 추세 변수를 생성하는 함수\n",
    "def calculate_monthly_avg_rainfall_and_trend(data):\n",
    "    # 구간 확률을 계산하여 데이터에 추가\n",
    "    segment_prob_df = calculate_segment_probabilities(data)\n",
    "    data = pd.concat([data, segment_prob_df], axis=1)\n",
    "    \n",
    "    # sum_segment_probs 계산\n",
    "    data['sum_segment_probs'] = data[['segment_prob_1', 'segment_prob_2', 'segment_prob_3', 'segment_prob_4', 'segment_prob_5', 'segment_prob_6', 'segment_prob_7', 'segment_prob_8', 'segment_prob_9']].sum(axis=1)\n",
    "    \n",
    "    # 월별 평균 강수 확률 계산\n",
    "    data['monthly_avg_rainfall_prob'] = data.groupby('ef_month')['sum_segment_probs'].transform('mean')\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 다중 임계값을 사용한 이진 변수 생성 함수\n",
    "def create_multithreshold_binary_features(data, thresholds):\n",
    "    for threshold in thresholds:\n",
    "        data[f'is_rain_{threshold}'] = (data['sum_segment_probs'] >= threshold).astype(int)\n",
    "    return data\n",
    "\n",
    "# 비 내릴 확률을 측정하는 변수를 생성하는 함수\n",
    "def create_rain_probability_feature(data, thresholds):\n",
    "    # 각 임계값에서 생성된 이진 변수들을 합산하여 비 내릴 확률을 측정하는 변수를 생성\n",
    "    data['rain_probability'] = data[[f'is_rain_{threshold}' for threshold in thresholds]].sum(axis=1)\n",
    "    # 역수 취하기\n",
    "    data['rain_probability_inverse'] = 1 / data['rain_probability']\n",
    "    return data\n",
    "\n",
    "def create_rain_nonrain_diff(data):\n",
    "    data['rain_nonrain_diff'] = data.apply(\n",
    "        lambda row: row['segment_prob_zero'] - row['sum_segment_probs']\n",
    "        if row['segment_prob_zero'] - row['sum_segment_probs'] > 0\n",
    "        else 1 / abs(row['segment_prob_zero'] - row['sum_segment_probs']) if row['segment_prob_zero'] - row['sum_segment_probs'] < 0\n",
    "        else 1,\n",
    "        axis=1\n",
    "    )\n",
    "    return data\n",
    "\n",
    "def create_combined_metric(data):\n",
    "    data['rain_combined_metric'] = data['rain_probability_inverse'] * data['rain_nonrain_diff']\n",
    "    return data\n",
    "\n",
    "# 중요한 특성 목록 정의\n",
    "important_columns = [ \n",
    "    'segment_prob_1','segment_prob_2','segment_prob_3',\n",
    "    'segment_prob_4','segment_prob_5','segment_prob_6',\n",
    "    'segment_prob_7','segment_prob_8','segment_prob_9',\n",
    "     'ef_hour_sin', 'ef_hour_cos', 'ef_hour',\n",
    "    'ef_month', 'monthly_avg_rainfall_prob', \n",
    "    'rain_combined_metric','fc_ef_day_diff','fc_ef_hour_diff',\n",
    "    'hourly_rain_prob_change_rate'#'is_rainy_season',\n",
    "]\n",
    "\n",
    "# 데이터 로드 및 전처리 함수\n",
    "def load_and_preprocess_data(db_path, table_name, column_names, thresholds, feature_names_path=None):\n",
    "    print(\"Loading and preprocessing data from the database.\")\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        data = pd.read_sql(query, conn)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from database: {e}\")\n",
    "        return None, None, None\n",
    "    finally:\n",
    "        conn.close()\n",
    "    \n",
    "    # 데이터가 제대로 로드되었는지 확인\n",
    "    if data is None or data.empty:\n",
    "        print(\"No data loaded from the database.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # 컬럼명 설정\n",
    "    data.columns = column_names\n",
    "    print(f\"Data shape after loading: {data.shape}\")\n",
    "    \n",
    "    # 연도 매핑\n",
    "    year_mapping = {'A': 1, 'B': 2, 'C': 3}\n",
    "    data['rainfall_train.fc_year'] = data['rainfall_train.fc_year'].map(year_mapping)\n",
    "    data['rainfall_train.ef_year'] = data['rainfall_train.ef_year'].map(year_mapping)\n",
    "    print(f\"Data shape after year mapping: {data.shape}\")\n",
    "\n",
    "    # 파생 변수 생성 (월별 평균 강수량 및 강수 추세)\n",
    "    data = calculate_monthly_avg_rainfall_and_trend(data)\n",
    "\n",
    "    # 다중 임계값을 사용한 이진 변수 생성\n",
    "    data = create_multithreshold_binary_features(data, thresholds)\n",
    "    \n",
    "    # 비 내릴 확률 측정 변수 생성\n",
    "    data = create_rain_probability_feature(data, thresholds)\n",
    "\n",
    "    # 새로운 변수 생성\n",
    "    data = create_rain_nonrain_diff(data)\n",
    "    \n",
    "    # 비가 내릴 확률과 비가 내리지 않을 척도를 결합한 변수 생성\n",
    "    data = create_combined_metric(data)\n",
    "\n",
    "    # 시간대별 강수확률을 구합니다.\n",
    "    data['hourly_rain_prob'] = data.groupby('ef_hour')['rain_probability'].transform('mean')\n",
    "    \n",
    "    # 시간대별 강수확률의 변화를 계산합니다.\n",
    "    data['hourly_rain_prob_change'] = data['hourly_rain_prob'].diff().fillna(0)\n",
    "    \n",
    "    # 시간대별 강수확률의 변화율을 계산합니다.\n",
    "    data['hourly_rain_prob_change_rate'] = data['hourly_rain_prob_change'] / data['hourly_rain_prob'].shift(1).fillna(1)\n",
    "    # 장마철\n",
    "    data['is_rainy_season'] = np.where(data['ef_month'].isin([6, 7, 8, 9]), 1, 0) \n",
    "\n",
    "    X = data[important_columns]\n",
    "    y = data['rainfall_train.class_interval']  # 타겟 변수 설정\n",
    "    print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "\n",
    "    # 정규화\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    print(\"Data loading and preprocessing completed.\")\n",
    "\n",
    "    # 피처 이름 저장\n",
    "    if feature_names_path:\n",
    "        feature_dir = os.path.dirname(feature_names_path)\n",
    "        if not os.path.exists(feature_dir):\n",
    "            os.makedirs(feature_dir)\n",
    "        with open(feature_names_path, 'wb') as f:\n",
    "            pickle.dump(important_columns, f)\n",
    "    \n",
    "    return X_scaled, y, data\n",
    "\n",
    "# 데이터베이스 경로 및 테이블 이름 설정\n",
    "db_path = 'C:/Users/82102/Desktop/rain/pyrain/data/qwer.db'\n",
    "table_name = 'qwert'\n",
    "\n",
    "# 열 이름 및 개수 가져오기\n",
    "column_names, column_count = get_column_names_and_count(db_path, table_name)\n",
    "if column_names:\n",
    "    # 임계값 목록\n",
    "    thresholds = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85]\n",
    "\n",
    "    # 데이터 로드 및 전처리\n",
    "    X, y, data = load_and_preprocess_data(db_path, table_name, column_names, thresholds)\n",
    "\n",
    "    print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve column names and count.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a1eeb-971e-4779-a039-564b4c298192",
   "metadata": {},
   "outputs": [],
   "source": [
    "전처리 과정에서 필요하다고 생각한 여러가지 파생변수를 생성하였다. 정규화진행 ,임계값부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f99268e-bdcd-4e29-a283-5d86cb4d2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "def calculate_vif(data):\n",
    "    print(\"Adding constant term to the data for VIF calculation.\")\n",
    "    # 데이터프레임에 상수항 추가\n",
    "    data = add_constant(data)\n",
    "    \n",
    "    print(\"Calculating VIF for each feature.\")\n",
    "    # VIF 계산\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"VIF Factor\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "    vif[\"features\"] = data.columns\n",
    "    \n",
    "    print(\"VIF calculation completed.\")\n",
    "    return vif\n",
    "\n",
    "# 'data'는 이전 코드에서 생성한 데이터프레임\n",
    "print(\"Extracting necessary data for VIF calculation.\")\n",
    "data_for_vif = data[important_columns]\n",
    "\n",
    "print(\"Starting VIF calculation process.\")\n",
    "vif_data = calculate_vif(data_for_vif)\n",
    "\n",
    "print(\"VIF Results:\")\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8aa1e4-76c4-41b6-8596-146bdd00f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif분석을 통하여 다중공선성 위험이 있는 변수들 vif 점수가 5점 이상인 변수들을 제거하였다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b23302d-bfe5-4e8f-ad61-148a0374b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    csi, hits, false_alarms, misses = calculate_csi(y_true, y_pred)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, balanced_accuracy, conf_matrix, csi, hits, false_alarms, misses\n",
    "\n",
    "# CSI 계산 함수\n",
    "def calculate_csi(y_true, y_pred):\n",
    "    hits = 0\n",
    "    false_alarms = 0\n",
    "    misses = 0\n",
    "    \n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == pred and true != 0:\n",
    "            hits += 1\n",
    "        elif true != pred and pred != 0 and true != 0:\n",
    "            false_alarms += 1\n",
    "        elif true != pred and true != 0 and pred == 0:\n",
    "            misses += 1\n",
    "            \n",
    "    csi = hits / (hits + false_alarms + misses) if (hits + false_alarms + misses) > 0 else 0\n",
    "    return csi, hits, false_alarms, misses\n",
    "\n",
    "# 데이터를 8:2로 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 가중치 적용 함수\n",
    "def apply_class_weights(y_true, weight_0_to_others, weight_others_to_0, weight_others_to_others):\n",
    "    weights = np.ones(y_true.shape[0])\n",
    "    for i, true_class in enumerate(y_true):\n",
    "        if true_class == 0:\n",
    "            weights[i] = weight_0_to_others\n",
    "        elif true_class != 0:\n",
    "            weights[i] = weight_others_to_0 if true_class != 0 else weight_others_to_others\n",
    "    return weights\n",
    "\n",
    "# 고정된 하이퍼파라미터\n",
    "n_estimators = 253\n",
    "max_depth = 39\n",
    "min_samples_split = 31\n",
    "min_samples_leaf = 2  # Assuming a typical value for min_samples_leaf\n",
    "\n",
    "# F1 스코어를 평가하는 함수\n",
    "def evaluate_with_criteria(y_true, y_pred):\n",
    "    accuracy, precision, recall, f1, balanced_accuracy, conf_matrix, csi, hits, false_alarms, misses = evaluate_model(y_true, y_pred)\n",
    "    if precision >= 2 and recall >= 2:\n",
    "        return f1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 가중치에 중점을 둔 최적화 함수\n",
    "def optimize_weights(weight_0_to_others, weight_others_to_0, weight_others_to_others):\n",
    "    sample_weights = apply_class_weights(y_train, weight_0_to_others, weight_others_to_0, weight_others_to_others)\n",
    "    \n",
    "    et_model = ExtraTreesClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    et_model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    y_pred = et_model.predict(X_test)\n",
    "    return evaluate_with_criteria(y_test, y_pred)\n",
    "\n",
    "# 가중치에 중점을 둔 베이지안 최적화\n",
    "pbounds_weights = {\n",
    "    'weight_0_to_others': (1, 6),\n",
    "    'weight_others_to_0': (9, 41),\n",
    "    'weight_others_to_others': (9, 31)\n",
    "}\n",
    "\n",
    "optimizer_weights = BayesianOptimization(\n",
    "    f=optimize_weights,\n",
    "    pbounds=pbounds_weights,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 최적화 수행\n",
    "optimizer_weights.maximize(\n",
    "    init_points=5,\n",
    "    n_iter=20\n",
    ")\n",
    "\n",
    "# 최적 가중치\n",
    "best_weights = optimizer_weights.max['params']\n",
    "print(f\"Best Weights: {best_weights}\")\n",
    "\n",
    "# 최적 가중치로 모델 학습\n",
    "sample_weights = apply_class_weights(y_train, best_weights['weight_0_to_others'], best_weights['weight_others_to_0'], best_weights['weight_others_to_others'])\n",
    "\n",
    "et_model_best = ExtraTreesClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=42\n",
    ")\n",
    "et_model_best.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# 예측\n",
    "y_pred_train_best = et_model_best.predict(X_train)\n",
    "y_pred_test_best = et_model_best.predict(X_test)\n",
    "\n",
    "# 평가\n",
    "train_results_best = evaluate_model(y_train, y_pred_train_best)\n",
    "test_results_best = evaluate_model(y_test, y_pred_test_best)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Train Results with Best Weights:\")\n",
    "print(f\"Accuracy: {train_results_best[0]:.4f}\")\n",
    "print(f\"Precision: {train_results_best[1]:.4f}\")\n",
    "print(f\"Recall: {train_results_best[2]:.4f}\")\n",
    "print(f\"F1 Score: {train_results_best[3]:.4f}\")\n",
    "print(f\"Balanced Accuracy: {train_results_best[4]:.4f}\")\n",
    "print(f\"CSI: {train_results_best[6]:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{train_results_best[5]}\")\n",
    "\n",
    "print(\"\\nTest Results with Best Weights:\")\n",
    "print(f\"Accuracy: {test_results_best[0]:.4f}\")\n",
    "print(f\"Precision: {test_results_best[1]:.4f}\")\n",
    "print(f\"Recall: {test_results_best[2]:.4f}\")\n",
    "print(f\"F1 Score: {test_results_best[3]:.4f}\")\n",
    "print(f\"Balanced Accuracy: {test_results_best[4]:.4f}\")\n",
    "print(f\"CSI: {test_results_best[6]:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{test_results_best[5]}\")\n",
    "\n",
    "# 변수 중요도\n",
    "importance_best = et_model_best.feature_importances_\n",
    "importance_df_best = pd.DataFrame({'Feature': X_train.columns, 'Importance': importance_best}).sort_values(by='Importance', ascending=False)\n",
    "print(\"Feature Importance:\")\n",
    "print(importance_df_best)\n",
    "\n",
    "# 모델 저장\n",
    "model_save_path_best = 'C:/Users/82102/Desktop/rain/pyrain/models/best_extra_trees_model_weights0.pkl'\n",
    "with open(model_save_path_best, 'wb') as f:\n",
    "    pickle.dump(et_model_best, f)\n",
    "print(f\"Best model saved to {model_save_path_best}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c3fc7-c7c3-4f50-a328-3d7f0d483273",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터를 8대2로 분리하고\n",
    "베이지안최적화는 파라미터의 어떤 지저믈 택하고 학습한 후 scoring(타겟)에 더 높은 점수를 위해 확률이 높은쪽으로 파라미터를 조정해준다??\n",
    "베이지안최적화를 이용하여 트리수,깊이,샘플스플릿을 결정해두었다. \n",
    "가중치란 잘못된예측에 대해 패널티를 부여하고 모델이 예측을 더 잘할수있도록 도와주는 역할을 한다. 데이터 불균형이 심각해 계급 0으로 편향된 예측을 하는 모델을 발견하고\n",
    "0_to_other, other_to_0, other_to_other 이 3개의 값을 베이지안최적화를 이용해서 다시 한번 최적화 해주었다.\n",
    "여기서 중요하게 생각한 점수는 f1,precision,recall 이 3개의 값의 최대치이면서 가장 균형이는 조합을 찾는게 목표였다.\n",
    "그 이유는 불균형 데이터에서의 accuracy와 csi 점수는 극단적인 scoring이라 판단하여 참고만하고 균형적인 예측을 위하여."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cba9c6-faaa-4c20-8ec9-3bbe6b6ef2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# 데이터 전처리 및 파생 변수 생성 함수\n",
    "def preprocess_and_create_features(data):\n",
    "    try:\n",
    "        print(\"Renaming columns...\")\n",
    "        data = data.rename(columns={\n",
    "            'rainfall_test.ef_month': 'ef_month',\n",
    "            'rainfall_test.fc_hour': 'fc_hour',\n",
    "            'rainfall_test.ef_hour': 'ef_hour',\n",
    "            'rainfall_test.fc_day': 'fc_day',\n",
    "            'rainfall_test.ef_day': 'ef_day',\n",
    "            'rainfall_test.fc_month': 'fc_month',\n",
    "            'rainfall_test.v01': 'v01',\n",
    "            'rainfall_test.v02': 'v02',\n",
    "            'rainfall_test.v03': 'v03', \n",
    "            'rainfall_test.v04': 'v04',\n",
    "            'rainfall_test.v05': 'v05',\n",
    "            'rainfall_test.v06': 'v06', \n",
    "            'rainfall_test.v07': 'v07',\n",
    "            'rainfall_test.v08': 'v08', \n",
    "            'rainfall_test.v09': 'v09',\n",
    "            'rainfall_test.dh':'dh'\n",
    "        })\n",
    "        print(\"Columns renamed.\")\n",
    "\n",
    "        print(\"Creating derived features...\")\n",
    "        data['fc_ef_day_diff'] = data['ef_day'] - data['fc_day']\n",
    "        data['ef_hour_sin'] = np.sin(2 * np.pi * data['ef_hour'] / 24)\n",
    "        data['ef_hour_cos'] = np.cos(2 * np.pi * data['ef_hour'] / 24)\n",
    "        data['fc_ef_day_ratio'] = np.where(data['ef_day'] != 0, data['fc_day'] / data['ef_day'], 0)\n",
    "        data['fc_ef_hour_diff'] = data['ef_hour'] - data['fc_hour']\n",
    "        data['fc_ef_hour_ratio'] = np.where(data['ef_hour'] != 0, data['fc_hour'] / data['ef_hour'], 0)\n",
    "        data['fc_ef_month_diff'] = data['ef_month'] - data['fc_month']\n",
    "\n",
    "        print(\"Derived features created.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing and creating features: {e}\")\n",
    "        return None\n",
    "\n",
    "# 구간 확률 계산 함수\n",
    "def calculate_segment_probabilities(data):\n",
    "    try:\n",
    "        print(\"Calculating segment probabilities...\")\n",
    "        segment_prob_1 = data['v01'] - data['v02']\n",
    "        segment_prob_2 = data['v02'] - data['v03']\n",
    "        segment_prob_3 = data['v03'] - data['v04']\n",
    "        segment_prob_4 = data['v04'] - data['v05']\n",
    "        segment_prob_5 = data['v05'] - data['v06']\n",
    "        segment_prob_6 = data['v06'] - data['v07']\n",
    "        segment_prob_7 = data['v07'] - data['v08']\n",
    "        segment_prob_8 = data['v08'] - data['v09']\n",
    "        segment_prob_9 = data['v09']\n",
    "        \n",
    "        segment_prob_df = pd.DataFrame({\n",
    "            'segment_prob_1': segment_prob_1,\n",
    "            'segment_prob_2': segment_prob_2,\n",
    "            'segment_prob_3': segment_prob_3,\n",
    "            'segment_prob_4': segment_prob_4,\n",
    "            'segment_prob_5': segment_prob_5,\n",
    "            'segment_prob_6': segment_prob_6,\n",
    "            'segment_prob_7': segment_prob_7,\n",
    "            'segment_prob_8': segment_prob_8,\n",
    "            'segment_prob_9': segment_prob_9\n",
    "        })\n",
    "        \n",
    "        segment_prob_df['segment_prob_zero'] = 100 - segment_prob_df[['segment_prob_1', 'segment_prob_2', 'segment_prob_3', 'segment_prob_4', 'segment_prob_5', 'segment_prob_6', 'segment_prob_7', 'segment_prob_8', 'segment_prob_9']].sum(axis=1)\n",
    "        \n",
    "        print(\"Segment probabilities calculated.\")\n",
    "        return segment_prob_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculating segment probabilities: {e}\")\n",
    "        return None\n",
    "\n",
    "# 월별 평균 강수량과 강수 추세 변수를 생성하는 함수\n",
    "def calculate_monthly_avg_rainfall_and_trend(data):\n",
    "    try:\n",
    "        print(\"Calculating monthly average rainfall and trend...\")\n",
    "        # 구간 확률을 계산하여 데이터에 추가\n",
    "        segment_prob_df = calculate_segment_probabilities(data)\n",
    "        if segment_prob_df is None:\n",
    "            return None\n",
    "        data = pd.concat([data, segment_prob_df], axis=1)\n",
    "        \n",
    "        # 중복 열 제거\n",
    "        data = data.loc[:, ~data.columns.duplicated()]\n",
    "        \n",
    "        # sum_segment_probs 계산\n",
    "        data['sum_segment_probs'] = data[['segment_prob_1', 'segment_prob_2', 'segment_prob_3', 'segment_prob_4', 'segment_prob_5', 'segment_prob_6', 'segment_prob_7', 'segment_prob_8', 'segment_prob_9']].sum(axis=1)\n",
    "        \n",
    "        # 월별 평균 강수 확률 계산\n",
    "        data['monthly_avg_rainfall_prob'] = data.groupby('ef_month')['sum_segment_probs'].transform('mean')\n",
    "        \n",
    "        print(\"Monthly average rainfall and trend calculated.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error in calculating monthly average rainfall and trend: {e}\")\n",
    "        return None\n",
    "\n",
    "# 다중 임계값을 사용한 이진 변수 생성 함수\n",
    "def create_multithreshold_binary_features(data, thresholds):\n",
    "    try:\n",
    "        print(\"Creating multi-threshold binary features...\")\n",
    "        for threshold in thresholds:\n",
    "            data[f'is_rain_{threshold}'] = (data['sum_segment_probs'] >= threshold).astype(int)\n",
    "        print(\"Multi-threshold binary features created.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error in creating multi-threshold binary features: {e}\")\n",
    "        return None\n",
    "\n",
    "# 비 내릴 확률을 측정하는 변수를 생성하는 함수\n",
    "def create_rain_probability_feature(data, thresholds):\n",
    "    try:\n",
    "        print(\"Creating rain probability feature...\")\n",
    "        data['rain_probability'] = data[[f'is_rain_{threshold}' for threshold in thresholds]].sum(axis=1)\n",
    "        data['rain_probability_inverse'] = 1 / data['rain_probability']\n",
    "        print(\"Rain probability feature created.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error in creating rain probability feature: {e}\")\n",
    "        return None\n",
    "\n",
    "# 비가 내리지 않을 척도 변수 생성 함수\n",
    "def create_rain_nonrain_diff(data):\n",
    "    try:\n",
    "        print(\"Creating rain non-rain difference...\")\n",
    "        diff = data['segment_prob_zero'] - data['sum_segment_probs']\n",
    "        data['rain_nonrain_diff'] = np.where(diff > 0, diff, 1 / np.abs(diff))\n",
    "        data['rain_nonrain_diff'] = np.where(diff == 0, 1, data['rain_nonrain_diff'])\n",
    "        print(\"Rain non-rain difference created.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error in creating rain non-rain difference: {e}\")\n",
    "        return None\n",
    "        \n",
    "def create_combined_metric(data):\n",
    "    data['rain_combined_metric'] = data['rain_probability_inverse'] * data['rain_nonrain_diff']\n",
    "    return data\n",
    "\n",
    "# 피처 정규화 함수\n",
    "def normalize_features(data, features):\n",
    "    try:\n",
    "        print(\"Normalizing features...\")\n",
    "        scaler = StandardScaler()\n",
    "        data[features] = scaler.fit_transform(data[features])\n",
    "        print(\"Features normalized.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalizing features: {e}\")\n",
    "        return None\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def preprocess_and_encode_test_data(data, feature_list, thresholds):\n",
    "    try:\n",
    "        print(\"Starting data preprocessing...\")\n",
    "        data = preprocess_and_create_features(data)\n",
    "        if data is None:\n",
    "            print(\"Failed at preprocessing and creating features.\")\n",
    "            return None\n",
    "\n",
    "        print(\"Calculating segment probabilities...\")\n",
    "        segment_prob_df = calculate_segment_probabilities(data)\n",
    "        if segment_prob_df is None:\n",
    "            print(\"Failed at calculating segment probabilities.\")\n",
    "            return None\n",
    "\n",
    "        # 중복 열 제거\n",
    "        segment_prob_df = segment_prob_df.loc[:, ~segment_prob_df.columns.duplicated()]\n",
    "        \n",
    "        data = pd.concat([data, segment_prob_df], axis=1)\n",
    "        \n",
    "        print(\"Calculating monthly average rainfall and trend...\")\n",
    "        data = calculate_monthly_avg_rainfall_and_trend(data)\n",
    "        if data is None:\n",
    "            print(\"Failed at calculating monthly average rainfall and trend.\")\n",
    "            return None\n",
    "\n",
    "        # 중복 열 제거\n",
    "        data = data.loc[:, ~data.columns.duplicated()]\n",
    "        \n",
    "        print(\"Creating multi-threshold binary features...\")\n",
    "        data = create_multithreshold_binary_features(data, thresholds)\n",
    "        if data is None:\n",
    "            print(\"Failed at creating multi-threshold binary features.\")\n",
    "            return None\n",
    "\n",
    "        # 중복 열 제거\n",
    "        data = data.loc[:, ~data.columns.duplicated()]\n",
    "        \n",
    "        print(\"Creating rain probability feature...\")\n",
    "        data = create_rain_probability_feature(data, thresholds)\n",
    "        if data is None:\n",
    "            print(\"Failed at creating rain probability feature.\")\n",
    "            return None\n",
    "\n",
    "        # 중복 열 제거\n",
    "        data = data.loc[:, ~data.columns.duplicated()]\n",
    "        \n",
    "        print(\"Creating rain non-rain difference...\")\n",
    "        data = create_rain_nonrain_diff(data)\n",
    "        if data is None:\n",
    "            print(\"Failed at creating rain non-rain difference.\")\n",
    "            return None\n",
    "            # 비가 내릴 확률과 비가 내리지 않을 척도를 결합한 변수 생성\n",
    "        print(\"Creating create_combined_metric...\")\n",
    "        data = create_combined_metric(data)\n",
    "        if data is None:\n",
    "            print(\"Failed at creating create_combined_metric.\")\n",
    "            return None\n",
    "\n",
    "        # 시간대별 강수확률을 구합니다.\n",
    "        data['hourly_rain_prob'] = data.groupby('ef_hour')['rain_probability'].transform('mean')\n",
    "        \n",
    "        # 시간대별 강수확률의 변화를 계산합니다.\n",
    "        data['hourly_rain_prob_change'] = data['hourly_rain_prob'].diff().fillna(0)\n",
    "        \n",
    "        # 시간대별 강수확률의 변화율을 계산합니다.\n",
    "        data['hourly_rain_prob_change_rate'] = data['hourly_rain_prob_change'] / data['hourly_rain_prob'].shift(1).fillna(1)\n",
    "        \n",
    "        # 장마철\n",
    "        data['is_rainy_season'] = np.where(data['ef_month'].isin([6, 7, 8, 9]), 1, 0) \n",
    "\n",
    "        print(\"Normalizing features...\")\n",
    "        features_to_normalize = [\n",
    "        'segment_prob_1','segment_prob_2','segment_prob_3',\n",
    "        'segment_prob_4','segment_prob_5','segment_prob_6',\n",
    "        'segment_prob_7','segment_prob_8','segment_prob_9',\n",
    "         'ef_hour_sin', 'ef_hour_cos', 'ef_hour',\n",
    "        'ef_month', 'monthly_avg_rainfall_prob', \n",
    "        'rain_combined_metric','fc_ef_day_diff','fc_ef_hour_diff',\n",
    "        'hourly_rain_prob_change_rate'\n",
    "        ]\n",
    "        data = normalize_features(data, features_to_normalize)\n",
    "        if data is None:\n",
    "            print(\"Failed at normalizing features.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"Removing duplicate columns...\")\n",
    "        data = data.loc[:, ~data.columns.duplicated()]\n",
    "        \n",
    "        X_test = data[feature_list]\n",
    "        \n",
    "        print(f\"Selected important columns for X_test with shape: {X_test.shape}\")\n",
    "\n",
    "        print(\"Data preprocessing completed.\")\n",
    "        return X_test\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing and encoding test data: {e}\")\n",
    "        return None\n",
    "\n",
    "# 예측 함수 정의\n",
    "def predict_with_rf(model_path, test_file_path, output_file_path):\n",
    "    try:\n",
    "        print(f\"Loading model from {model_path}...\")\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(\"Model loaded successfully.\")\n",
    "        \n",
    "        print(f\"Loading test data from {test_file_path}...\")\n",
    "        original_test_data = pd.read_csv(test_file_path)\n",
    "        print(f\"Test data shape: {original_test_data.shape}\")\n",
    "        \n",
    "        feature_list = [\n",
    "        'segment_prob_1','segment_prob_2','segment_prob_3',\n",
    "        'segment_prob_4','segment_prob_5','segment_prob_6',\n",
    "        'segment_prob_7','segment_prob_8','segment_prob_9',\n",
    "         'ef_hour_sin', 'ef_hour_cos', 'ef_hour',\n",
    "        'ef_month', 'monthly_avg_rainfall_prob', \n",
    "        'rain_combined_metric','fc_ef_day_diff','fc_ef_hour_diff',\n",
    "        'hourly_rain_prob_change_rate'\n",
    "        ]\n",
    "\n",
    "        thresholds =[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85]\n",
    "        \n",
    "        test_data = preprocess_and_encode_test_data(original_test_data, feature_list, thresholds)\n",
    "        if test_data is None:\n",
    "            print(\"Failed to preprocess and encode test data.\")\n",
    "            return\n",
    "        \n",
    "        print(\"Features used for prediction:\")\n",
    "        print(test_data.columns.tolist())\n",
    "        \n",
    "        print(\"Performing prediction...\")\n",
    "        y_pred = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Prediction completed with shape: {y_pred.shape}\")\n",
    "\n",
    "        na_indices = original_test_data['rainfall_test.class_interval'].isnull()\n",
    "        original_test_data.loc[na_indices, 'rainfall_test.class_interval'] = y_pred[na_indices]\n",
    "\n",
    "        print(f\"Saving predicted results to {output_file_path}...\")\n",
    "        original_test_data.to_csv(output_file_path, index=False)\n",
    "        print(f\"Predicted results saved to {output_file_path} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction process: {e}\")\n",
    "\n",
    "# 실행 (모델 불러오기 및 예측)\n",
    "model_path = 'C:/Users/82102/Desktop/rain/pyrain/models/best_extra_trees_model_bayesianf1.pkl'\n",
    "test_file_path = 'C:/Users/82102/Desktop/rain/pyrain/data/rainfall_test3.csv'\n",
    "output_file_path = 'C:/Users/82102/Desktop/rain/pyrain/240062.csv'\n",
    "\n",
    "predict_with_rf(model_path, test_file_path, output_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
